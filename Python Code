import numpy
import pandas as pd
import matplotlib.pyplot as plt
import re
import nltk
import urllib.request
import wordcloud
from wordcloud import STOPWORDS,WordCloud
import spacy
from nltk.tokenize import word_tokenize
from nltk.tag import pos_tag
import spacy
from spacy import displacy
from collections import Counter
import spacy
import en_core_web_sm
nlp = en_core_web_sm.load()
from spacy import displacy

url1 = "http://www.gutenberg.org/files/63396/63396-0.txt"
url2 = "http://www.gutenberg.org/files/63397/63397-0.txt"
raw1 = urllib.request.urlopen(url1).read()
raw2 = urllib.request.urlopen(url2).read()
# converting byte type into string type
T1 = raw1.decode('utf-8')
T2 = raw2.decode('utf-8')
# simple pre-processing steps

# removing index and reference part
T1 = T1.split("INTRODUCTION", 1)[0]
T1 = T1.split("SOME SOURCE REFERENCES", 1)[0]
T2 = T2.split("PREFACE", 1)[0]
T2 = T2.split("BIBLIOGRAPHY", 1)[0]

# converting all into lower case
T1 = T1.lower()
T2 = T2.lower()

# tokenizing the words
t1 = nltk.word_tokenize(T1)
t2 = nltk.word_tokenize(T2)


# analyzing frequency distribution of both tokens t1 and t2
word_counts1 = nltk.Counter(t1)
word_counts2 = nltk.Counter(t2)
df = pd.DataFrame.from_dict(word_counts1, orient='index')
df.plot(logy=True, kind='bar', figsize=(20, 10), title='Frequency Distribution of token t1').legend(["Frequency"])
df = pd.DataFrame.from_dict(word_counts2, orient='index')
df.plot(logy=True, kind='bar', figsize=(20, 10), title='Frequency Distribution of token t2').legend(["Frequency"])

# relationship between the word length and frequency for both T1 and T2
f1 = {}
for i in list(word_counts1.elements()):
    val = len(i)
    if val in f1:
        f1[val] += word_counts1[i]
    else:
       f1[val] = word_counts1[i]
f2 = list()
for i in range(1, 34):
    if i in f1:
       f2.append((i, f1[i]))

df3 = pd.DataFrame.from_dict(f2)
df3.plot(logy=True, x=0, y=1, kind='bar', figsize=(20, 10), title='Word Length vs Frequency Distribution of t1', legend=False).\
    set(xlabel="Length of Words", ylabel="Frequency")

f3 = {}
for i in list(word_counts2.elements()):
    val = len(i)
    if val in f3:
        f3[val] += word_counts2[i]
    else:
       f3[val] = word_counts2[i]
f4 = list()
for i in range(1, 34):
    if i in f4:
       f4.append((i, f4[i]))

df4 = pd.DataFrame.from_dict(f2)
df4.plot(logy=True, x=0, y=1, kind='bar', figsize=(20, 10), title='Word Length vs Frequency Distribution of t2', legend=False).\
    set(xlabel="Length of Words", ylabel="Frequency")

# creating word cloud of both T1 and T2
wc1 = WordCloud(background_color="black", max_words=200, width=400, height=400, stopwords=None).generate(T1)
plt.imshow(wc1)
plt.show()
wc2 = WordCloud(background_color="black", max_words=200, width=400, height=400, stopwords=None).generate(T2)
plt.imshow(wc2)
plt.show()

# creating word cloud of both T1 and T2 after removing stopwords
word_cloud1 = WordCloud(background_color="white", max_words=200, width=400, height=400, stopwords=STOPWORDS).\
    generate(T1)
plt.imshow(word_cloud1)
plt.show()

word_cloud2 = WordCloud(background_color="white", max_words=200, width=400, height=400, stopwords=STOPWORDS).\
    generate(T2)
plt.imshow(word_cloud2)
plt.show()


# pos tagging of both t1 and t2
p1 = nltk.pos_tag(t1)
p2 = nltk.pos_tag(t2)

f5 = {}
for i in p1:
 val = i[1]
 if val in f5:
     f5[val] += 1
 else:
     f5[val] = 1
cnt = list()
for i in list(f5.keys()):
    cnt.append((str(i), f5[str(i)]))
df5 = pd.DataFrame.from_dict(cnt)
df5.plot(logy=True, x=0, y=1, kind='bar', figsize=(20, 10), title='Tags vs Distribution of T1', legend=False).\
    set(xlabel="Tags", yLabel="Frequency")

for i in p2:
 val = i[1]
 if val in f5:
     f5[val] += 1
 else:
     f5[val] = 1
cnt = list()
for i in list(f5.keys()):
    cnt.append((str(i), f5[str(i)]))
df5 = pd.DataFrame.from_dict(cnt)
df5.plot(logy=True, x=0, y=1, kind='bar', figsize=(20, 10), title='Tags vs Distribution of T2 ', legend=False).\
    set(xlabel="Tags", yLabel="Frequency")
plt.show()



# For the noun and verb labelling for the  first novel
dict1={'NN':0,'NNP':0,'NNS':0,'NNPS':0}
dict2={'VB':0,'VBN':0,'VBP':0,'VBD':0,'VBZ':0,'VBG':0,'VBN':0 }
count=0
for sentence in t1:
     for word,pos in nltk.pos_tag(nltk.word_tokenize(str(sentence))):
      if (pos == 'NN' or pos == 'NNP' or pos == 'NNS' or pos == 'NNPS'):
            count=dict1[pos]
            count+=1
            dict1[pos]=count
      if (pos == 'VB' or pos == 'VBN' or pos == 'VBP' or pos == 'VBD'or pos=='VBZ'or pos=='VBG'or pos=='VBN'):
            count=dict2[pos]
            count+=1
            dict2[pos]=count
           
plt.bar(list(dict1.keys()), dict1.values(), color='g')
plt.bar(list(dict2.keys()), dict2.values(), color='b')
plt.show() 

# For the noun and verb labelling for the second novel
dict1={'NN':0,'NNP':0,'NNS':0,'NNPS':0}
dict2={'VB':0,'VBN':0,'VBP':0,'VBD':0,'VBZ':0,'VBG':0,'VBN':0 }
count=0
for sentence in t2:
     for word,pos in nltk.pos_tag(nltk.word_tokenize(str(sentence))):
      if (pos == 'NN' or pos == 'NNP' or pos == 'NNS' or pos == 'NNPS'):
            count=dict1[pos]
            count+=1
            dict1[pos]=count
      if (pos == 'VB' or pos == 'VBN' or pos == 'VBP' or pos == 'VBD'or pos=='VBZ'or pos=='VBG'or pos=='VBN'):
            count=dict2[pos]
            count+=1
            dict2[pos]=count
           
plt.bar(list(dict1.keys()), dict1.values(), color='g')
plt.bar(list(dict2.keys()), dict2.values(), color='b')
plt.show() 


doc = nlp('During the Pleistocene Epoch or Great Ice Age, huge glaciers formed inCanada and advanced southward into the great, central, low-lyingInterior Plains of the United States. (See figure 2.) These glaciers andtheir deposits modified the surface of the land they covered, mostlybetween the Missouri and the Ohio Rivers; they smoothed the contours andgave the land a more subdued aspect than it had before they came. Thisglacially smoothed and modified land is called the Central Lowland.Although the ice sheets lapped onto the northern part, the Great Plainsis the largely unglaciated region that extends from the Gulf CoastalPlain in Texas northward into Canada between the Central Lowland and thefoot of the Rocky Mountains. Its eastern margin in Texas and Oklahoma ismarked by a prominent escarpment, the Caprock escarpment. Its southernmargin, where it abuts the Coastal Plain in Texas, is at another abruptrise or scarp along the Balcones fault zone.')
#Printing  the entities present in the given text
print([(X.text) for X in doc.ents])
#Printing  the entities tag labels  present in the given text
print([(X.label_) for X in doc.ents])




doc = nlp("For instance, the westbound traveler on Interstate Highway 70 traverses nearly a thousand miles of low, rounded hills after leaving theAppalachians; the rolling landscape is broken only by a few flat areaswhere glacial ice or small lakes once stood. Suddenly, near Salina,Kans., the observant traveler senses a difference in the landscape.Instead of rounded hills, widely or closely spaced, he sees on theskyline flat surfaces, or remnants of flat surfaces. As he climbs gentlywestward these broken horizontal lines stand etched against the sky.About 35 miles west of Salina he finds himself on a broad, flat plateau,here seemingly he can see forever. True, in places he descends intostream valleys, but only briefly, for he soon climbs back onto the flatsurface.")
displacy.render(doc, style="dep", jupyter=True, options={"distance":100})
displacy.render(doc, style="ent", jupyter=True)
